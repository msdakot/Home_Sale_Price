---
title: "Hedonic Home Price Prediction in Boston, MA"
author: "Dhruvi kothari"
date: "October 25, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Background

*Note: This was a fictional client based machine learnt predictive model prepared as a class assignment at University of Pennsylvania


Zillow provides data on over 110 million homes in the United States, including home sale value estimates, making its ability to accurately predict housing prices critical. But recently the company's inaccuracies of sale values have led to negative press for the company, and created opportunities for competitors to gain advantage in the market. Because Zillow’s data is so comprehensive, it has also become a valuable tool for planning and policy. Accurately predicting home prices is difficult. Home prices are a function of many complex considerations including the physical characteristics, location, and other factors including seasonal variations and the personal preferences of buyers and sellers. 
To achieve better prediction accuracy, I have collected data to capture as many of the factors listed above as possible. More specifically, I worked to capture the variation in prices due to both physical characteristics of homes, and the underlying spatial patterns of prices in the region. The resulting model was able to improve predictions considerably, with a new error margin of only about 11%-12%.


```{r message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(leaflet)
library(stargazer)
library(corrplot)
library(reshape2)
library(ggplot2)
library(ggmap)
```


The map below shows the observation points in the data set. 

```{r message=FALSE, warning=FALSE}
options(scipen=999)

df <- read.csv("Data.csv")
df2 <- dplyr::select(df, -Parcel_No, -Latitude_1, -Longitud_1)
df$SalePrice <- sapply(df$SalePrice, as.numeric)

df_Train <- df2[df2$SalePrice > 0,]
  
df_Test <- df2[df2$SalePrice == 0,]
```

```{r fig.height=6, fig.width=9}

df_Train1<-df[df$SalePrice>0,]
df_Train1$SP_1000<-df_Train1$SalePrice/10000

pal <- colorNumeric(
  palette = c("yellow","#F08F04","#F0433A", "maroon"),
  domain = df_Train1$SP_1000)

leaflet(df_Train1) %>% setView(lng = -71.08, lat = 42.33, zoom = 12) %>% addProviderTiles(providers$CartoDB.DarkMatter)%>%
  addCircles(lng = df_Train1$Longitud_1, lat = df_Train1$Latitude_1, 
             radius = ~(df_Train1$SP_1000), color = pal(df_Train1$SP_1000),
             weight=1, popup =  ~paste(Parcel_No, Neighborhood, sep = ",")) %>%
  addLegend("topright", pal=pal, values=df_Train1$SP_1000,
    title = "Training/Testing Set",
    opacity = 1)
```


###Data

Data were collected from three different sources: 1) Sales data and home characteristics came from 2015 home sales data from the Suffolk County Massachusetts Assessor’s Office.Physical housing characteristics, and the sales date were provided in the original Zillow data set. Some variables from the original data set with many categorical values or uneven distribution of values (e.g. sale date, year remodeled) were modified in order to increase predictive capacity.

Local amenities and undesirable attributes data came from Analyze Boston, the city of Boston’s open data portal. Data collection focused on data representing spatial objects (e.g. open spaces) and events (e.g., crime); 3) Socio-economic data came from the 2015 American Community Survey Five Year Estimates. Data collection focused on data that would be more likely to reflect community characteristics to capture some of the spatial clustering. 
Table below displays the names and descriptions of all variables and Table 2 provides the summary statistics.


###Summary Statistics
Description of the variables is available in the Appendix.  

```{r message=FALSE, warning=FALSE}
stargazer(df_Train, type="text", title = "Variable Summary Statistics") 
```


###Correlation Matrix
The figure belows shows the correlation between various explanatory variables. Eliminating multi-collinearity (high level of correlation between predictors) can not only help eliminating double counting, but also helps improve the accuracy of the overall model. Therefore parameters showing correlation greater than 0.8 were determined to be removed from the model. 


```{r fig.height=15, fig.width=15, message=FALSE, warning=FALSE}
dfNum <- dplyr::select(df_Train, -UniqueSale, -SaleSeason, -STRUCTURE_, -R_BLDG_STY, -SaleMonth, -Style , -LU, -R_ROOF_TYP, -HEAT_SYS, -R_EXT_FIN, -Neighborhood)

#Correlation Matrix
CorMatrix <- cor(dfNum)


corrplot(CorMatrix, method = "color", order = "AOE", addCoef.col="grey", type = "upper", addCoefasPercent = FALSE, number.cex = .7)

```

```{r message=FALSE, warning=FALSE}
#Removing multi-colinear variables (>.80)
df2 <- dplyr::select(df2, -GROSS_AREA, -LivingArea, -YR_REMOD, -R_BDRMS, -R_KITCH, -Dist_AP)
df_Train <- df2[df2$SalePrice > 0,]
df_Test <- df2[df2$SalePrice == 0,]
```

###Home Sales 
```{r message=FALSE, warning=FALSE}
baseMap <- get_map(location = "Boston", 
                   source = "stamen", 
                   zoom = 11, 
                   maptype= 'toner')

invert <- function(x) rgb(t(255-col2rgb(x))/255)    
baseMap_invert <- as.raster(apply(baseMap, 2, invert))
class(baseMap_invert) <- class(baseMap)
attr(baseMap_invert, "bb") <- attr(baseMap, "bb")

```
The map below visualizes distribution of the boston property sales based on the price it was sold. The map clearly shows spatial correlations, and therefore needs an addition of control measure to account for such a pattern. 

```{r message=FALSE, warning=FALSE}
ggmap(baseMap_invert) + 
  geom_point(data = df_Train1, aes(x=Longitud_1, y=Latitude_1, 
                                              colour=ntile(SP_1000, 5)),
             size=0.5, alpha=0.5) +
  scale_colour_gradient(low ="yellow", high = "maroon", space = "Lab", 
                        name="Sale Price") + 
  theme_void()
```


```{r message=FALSE, warning=FALSE}
#independent variables 
```


###Analyzing Predictor Distributions

The OLS model works best when the predictors and dependent variable are normally distributed. The distribution of some predictors may be brought closer to a normal distribution by log-transforming them. The plots below show the current distributions of the continuous predictors. 
The linear regression model works accurately when the parameters and predictor variables are normally distributed. Plotting the dependent variable, as shown in the histogram, the predictor variable had some logirithmic distribution. So for the purpose of this analysis, dependent variable and other non-normally distributed independent variables were logirithmically transformed to fit a normal distribution. 


```{r fig.height=20, fig.width=20, message=FALSE, warning=FALSE}
#Analysis of continuous predictors
dfCont <- dplyr::select(dfNum, -GROSS_AREA, -LivingArea, -YR_REMOD, -NewlyRemodeled, -R_BDRMS, -R_KITCH, -Dist_AP,
                        -NearCommonwealth, -NearImpBldg, -NearUni, -C_AC, -OWN_OCC, -PTYPE, -ZIPCODE, 
                        -R_FPLACE, -R_HALF_BTH, -R_FULL_BTH, -NearAP)
```


Predictors which are negatively skewed are the best candidates for normalizing by using log-transformation. The new distributions of the transformed predictors are shown below. 


```{r fig.height=5, fig.show='hold', fig.width=5, message=FALSE, warning=FALSE, out.width='50%'}
#Log-Transforming to normalize selected predictors 
ggplot(df2, aes(x=LAND_SF)) + geom_histogram(fill="maroon")
ggplot(df2, aes(x=log(LAND_SF))) + geom_histogram(fill="maroon")
```
```{r message=FALSE, warning=FALSE, include=FALSE}
df2$LogLAND_SF <- log(df2$LAND_SF)
df2$LAND_SF <- NULL

df2$LogLIVING_ARE <- log(df2$LIVING_ARE)
df2$LIVING_ARE <- NULL

df2$LogPCTVACANT <- log(df2$PCTVACANT + 1)
df2$PCTVACANT <- NULL

df2$LogDist_Major_Road <- log(df2$Dist_Major_Road + 1)
df2$Dist_Major_Road <- NULL

df2$LogAve_SalePr <- log(df2$Ave_SalePr)
df2$Ave_SalePr <- NULL

df_Train <- df2[df2$SalePrice > 0,]
df_Test <- df2[df2$SalePrice == 0,]
```

### Methods

To generate the price predictions, I will use a Hedonic OLS Regression model. This method evaluates the direction and strength of the relationship between the dependent variable in question (home prices) and the many factors (predictors) which may affect it. The model can estimate the effect of each of our predictors on sale prices while holding all other predictors constant, thereby allowing us to consider the effect of different variables concurrently.

To train the model, I created a training data set (data set with known sale prices) which included 1323 observations to "train" the regression model to predict the home sales price in the test data set (the data set of homes with unknown, or 0, prices). Using this training data, it is possible to calibrate the regression coefficients to model home prices based on the data. Lastly, I used this "trained" model to predict the sale prices in the test set. 

In evaluating the predictive ability of our model, I took two separate approaches. The first approach was "In-sample training", in which I divided the training data-set into two groups, and used one of the groups to predict the prices in the other. The second approach was a 10-fold cross-validation algorithm, which randomly divids the training set into ten equal "folds", and one by one predicts prices for each of the folds using the remaining nine folds. 

To examine whether the model was sufficiently capturing spatial structure of prices, I used the Moran's I method. This method evaluates whether the model's errors are clustered in space to a statistically-significant degree (which would indicate some spatial dynamic that was not accounted for in the model). 


### Model Building

The model-building process is shown below. 

####Linear Regression model 1: All predictors
```{r }
reg1 <- lm(log(SalePrice) ~ ., data =  df_Train %>% 
             as.data.frame %>% dplyr::select(-UniqueSale))

```

####Stepwise Variable Analysis:
```{r message=FALSE, warning=FALSE, include=FALSE}
library(MASS)
library(spdep)
step <- stepAIC(reg1, direction="both")
```

```{r}
step$anova
```

####linear regression model 2- Removing more insignificant predictors:
```{r message=FALSE, warning=FALSE}

reg3 <- lm(log(SalePrice) ~ ., data =  df_Train %>% 
             as.data.frame %>% dplyr::select(-UniqueSale, -LogDist_Major_Road, -PCTOWNEROC, -DistToPoor,
                                             -DistToCBD, -SchoolGrade, -MEDHHINC, -WalkScore, -TransitSco, 
                                             -BikeScore, -FeetToParks, -HEAT_SYS, -R_EXT_FIN, -R_BLDG_STY,
                                             -R_ROOF_TYP,
                                             -STRUCTURE_,-OWN_OCC, -ZIPCODE, -Style, -SaleMonth, -YR_BUILT,
                                             -R_TOTAL_RM,
                                             -NearImpBldg, -NearCommonwealth,
                                             -NearCommonwealth, -LogPCTVACANT))


stargazer(reg1, reg3, type="text", align=TRUE, no.space=TRUE, single.row=TRUE, ci=FALSE, column.labels=c("Model 1","Model 2", "Model 3" ), title="Comparision of the models")

```

####Stepwise Variable Analysis 2: 
```{r message=FALSE, warning=FALSE, include=FALSE}
step <- stepAIC(reg3, direction="both")
```
```{r}
step$anova
```

###Model Evaluation and Assumption Testing

The following section will test the assumptions associated with the OLS model (ie. Residual normality, heteroscedasticity). 

####Testing Residual Distribution:
```{r}
Reg_Dataframe <- cbind(reg3$residuals,reg3$fitted.values)
Reg_Dataframe <- as.data.frame(Reg_Dataframe)

colnames(Reg_Dataframe) <- c("residuals", "predictedValues")


ggplot(reg3, aes(Reg_Dataframe$residuals)) + 
  geom_histogram(bins=25, fill="maroon") +
  labs(x="Residuals",y="Count")+
  theme_minimal()
```

/nThe residuals are normally distributed, which means that the model is able to explain almost all the variation present in the dataset. 

####Testing for Heteroscedasticity:

Predicted as function of residuals:
The error values are not only randomly distributed as shown in the figure below, but 
```{r}
ggplot(data = Reg_Dataframe, aes(x = residuals , y = predictedValues)) +
  geom_point(size = 0.7,color="maroon" ) + xlab("Residuals") + ylab("Predicted Values") + 
  ggtitle("Residual Values vs. Predicted Values") +  
  theme(plot.title = element_text(hjust = 0.5))
```

Predicted as function of observed:
```{r}
regDF <- cbind(log(df_Train$SalePrice), reg3$fitted.values)
colnames(regDF) <- c("Observed", "Predicted")
regDF <- as.data.frame(regDF)
ggplot() + 
  geom_point(data=regDF, aes(Observed, Predicted), color="maroon") +
  stat_smooth(data=regDF, aes(Observed, Observed), method = "lm", size = 1,fullrange = TRUE, colour = "#333333") + 
  labs(title="Predicted Price as a function\nof Observed Price") +
  theme(plot.title = element_text(hjust = 0.5))
```


the predicted values of the sale prices are almost linear to the observed sale prices, which shows that the model's errors are sufficiently homoscedastic in nature.  

####Mapping Residuals
```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
reg_residuals <- data.frame(reg3$residuals)
LonLat <- data.frame(df[df$SalePrice>0,]$Longitud_1, df[df$SalePrice>0,]$Latitude_1)
residualsToMap <- cbind(LonLat, reg_residuals )
colnames(residualsToMap) <- c("longitude", "latitude", "residual")
names(residualsToMap)

  
pal1 <- colorNumeric(
  palette = c("yellow","#F08F04", "maroon"),
  domain = residualsToMap$residual)
 
leaflet(residualsToMap) %>% setView(lng = -71.08, lat = 42.33, zoom = 12) %>% addProviderTiles(providers$CartoDB.DarkMatter)%>%
  addCircles(lng = residualsToMap$longitude, lat = residualsToMap$latitude, 
             radius = ~(residualsToMap$residual*500), color = pal1(residualsToMap$residual),
             weight=1) %>%
  addLegend("topright", pal=pal, values=residualsToMap$residual*500,
    title = "Residuals",
    opacity = 1) 
 
 
```

####Mapping Residuals (Raster Grid)
```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
Raster <-
  ggmap(baseMap_invert) +
  stat_summary_2d(geom = "tile",
                  bins = 80,
                  data=residualsToMap,
                  aes(x = longitude, y = latitude, z = ntile(residual,5))) +
                  scale_fill_gradient(low = "yellow", high = "maroon", 
                  guide = guide_legend(title = "Residuals \n (Quintiles)")) +
                  labs(title="Prediction Residuals (Raster Grid)") + theme_void()

Raster

```


The predicted errors looks spatially distributed and there is noticeable spatial pattern. 

####Moran's I Analysis
```{r message=FALSE, warning=FALSE}

coords <- cbind(df[df$SalePrice>0,]$Longitud_1, df[df$SalePrice>0,]$Latitude_1)
spatialWeights <- knn2nb(knearneigh(coords, 4))
moran.test(reg1$residuals, nb2listw(spatialWeights, style="W"))

```

Test results indicate that no significant spatial autocorrelation is present in the residuals. 


###In-Sample Training
```{r message=FALSE, warning=FALSE}
library(caret)
inTrain <- createDataPartition(
  y = df_Train$Neighborhood, 
  p = .75, list = FALSE)

IST.training <- df_Train[ inTrain,] #the in-sample training set
IST.test <- df_Train[-inTrain,]  #the in-sample test set

reg4 <- lm(log(SalePrice) ~ ., data =  IST.training%>% #regression with in-sample training data
             as.data.frame %>% dplyr::select(-UniqueSale, -LogDist_Major_Road, -PCTOWNEROC, -DistToPoor,
                                             -DistToCBD, -SchoolGrade, -MEDHHINC, -WalkScore, -TransitSco, 
                                             -BikeScore, -FeetToParks, -HEAT_SYS, -R_EXT_FIN, -R_BLDG_STY,
                                             -R_ROOF_TYP, -STRUCTURE_,
                                             -OWN_OCC, -ZIPCODE, -Style, -SaleMonth, -YR_BUILT, -R_TOTAL_RM,
                                             -NearImpBldg, -NearCommonwealth,
                                             -NearCommonwealth, -LogPCTVACANT, -LogDist_Major_Road)) 

#predict on in-sample test set
reg4Pred <- predict(reg4, IST.test)

reg4PredValues <- 
  data.frame(observedPrice = IST.test$SalePrice,
             predictedPrice = exp(reg4Pred))

#store predictions, observed, absolute error, and percent absolute error
reg4PredValues <-
  reg4PredValues %>%
  mutate(error = predictedPrice - observedPrice) %>%
  mutate(absError = abs(predictedPrice - observedPrice)) %>%
  mutate(percentAbsError = abs(predictedPrice - observedPrice) / observedPrice) 


stargazer(reg4PredValues, type = 'text')
```

####Testing Generalizability: 10-Fold Cross-Validation Method 
```{r message=FALSE, warning=FALSE}
fitControl <- trainControl(method = "cv", number = 10)

set.seed(825) #set seed for random number generator

lmFit <- train(log(SalePrice) ~ ., data = df_Train, 
               method = "lm", 
               trControl = fitControl)

lmFit$resample

stargazer(lmFit$resample, type = "text")
```

The plot below shows the distibution of mean absolute error (MAE) values for the 10 folds. 
```{r}
#Evaluating Generalizeability: Fold MAE Frequency Plot
ggplot(as.data.frame(lmFit$resample), aes(MAE)) +
  geom_histogram(bins=5, fill="maroon") +
  labs(x="Mean Absolute Error",
       y="Count")

```

No significant outliers are present 

While the model shows some strong potential (e.g., not much change in the MAPE and MAE from the training to the test data sets) further assessment of the model’s generalizability indicates it has some shortcomings.

As demonstrated by the random sample cross validation, while the model captures much of the variability in sales price for most samples, it does not do so for all. The variability of MAPE by neighborhood, as evidenced in both the map of the MAPE by neighborhood and the spatial cross validation, shows that the model does not predict equally well for all neighborhoods. While the model is fairly generalizable to middle-income neighborhoods, it is less so for low-income neighborhoods and not particularly generalizable to high-income neighborhoods. As noted, this could be because the factors include in the model are less relevant to high and low income neighborhoods.

###Conclusion
In conclusion, the recommendation for Zillow would be to allow for further refinement of the model. While the model predicts well for some neighborhoods, particularly middle-income neighborhoods, it does not predict as well for low and high income neighborhoods and as noted above, it fails to capture some of the spatial autocorrelation of homesales. The model could be improved by the addition of more data capturing spatial relationships (e.g., the mean home sales value of the 10 other nearest home sales) and tax assessment data.


###Appendix

####Variable Descriptions

**NewlyRemodeled** - Whether the unit was remodeled since 2005 (binary variable)

**GROSS_AREA** - Gross floor area of the unit

**NUM_FLOORS** - Number of floors 

**R_TOTAL_RM** - Total number of rooms

**R_BDRMS** - Number of bedrooms

**R_FULL_BTH** - Number of full bathrooms

**R_HALF_BTH** - Number of half bathrooms

**R_KITCH** - Number of kitchens in the structure

**R_FPLACE** - Number of fireplaces in the structure

**SaleSeason** - Season in which the sale took place. Summer= Jun-Aug, Fall = Sep-Nov, Winter = Dec-Feb, Spring= Mar-May

**Style** - Architectural style

**LU** - City's land use designation 

**R_ROOF_TYP** - Roof structure type: F Flat L Gambrel S Shed G Gable M Mansard H Hip O Other

**R_EXT_FIN** - Exterior finish type: A Asbestos K Concrete U Aluminum B Brick/Stone M Vinyl V Brick/Stone Veneer C Cement Board O Other W Wood Shake F Frame/Clapboard P Asphalt G Glass S Stucco

**C_AC** - Presence of central air-conditioning (binary)

**FeetToParks** - Distance to nearest park (feet)

**FeetToMetro** - Distance to nearest transit stop (feet)

**MEDHHINC** - Median household income of census block group

**PCTBACHMOR** - Percent of population in block group with bachelor's degree or more

**PCTWHITE** - Percent of population in block group which identify as white

**CrimeIndex** - Crime ranking based on density of violent crime occurrences in 2015 (1-6)

**NearUni** - within one kilometer of university (binary)

**SchoolGrade** - ranking of nearest public school (1-9) from greatschools.com

**DistToCBD** - Distance to Central Business District (feet)

**NearImpBldg** - Whether near important landmark/building (binary)

**NearCommonwealth** - whether near commonwealth avenue or Boston commons (binary)

**DistToPoor** - Distance to neighborhoods with median household income less than 25k

**NearAP** - whether within 1500 feet of Logan Airport

**Ave_SalePr** - Average sale price of 5 nearest homes

**Dist_SC** - Distance to public schools (feet)

**Dist_Major_Road** - Distance to road with speed limit over 35 mph (feet)

**LivingArea** - Net living area in unit in feet (logged in model)

**LAND_SF** - Size of lot in feet (logged in model)
